{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOrZgE5q/VVnfsmJ9N2I1lM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CleberJesus/machine_learning/blob/main/machine_learning_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Implementando uma Deep Learning do zero em python**"
      ],
      "metadata": {
        "id": "fw8B0rS42V8_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "QnhAJc2F2PJ3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as f\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from  time import time\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn, optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.ToTensor()\n",
        "\n",
        "#carrega a parte de treino do dataset\n",
        "trainset = datasets.MNIST('./MNIST_data/', download=True, transform=transform)\n",
        "#cria um buffer para pegar os dados por partes\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "#carrega a parte de validação\n",
        "valset = datasets.MNIST('./MNIST_data/', download=True, transform=transform)\n",
        "#cria um buffer para pegar os dados por\n",
        "valloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "mTXW5h8_32zC"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataiter = iter(trainloader)\n",
        "imagens, etiquetas = dataiter.next()\n",
        "plt.imshow(imagens[0].numpy().squeeze(), cmap='gray_r')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "G-Detct633Dh",
        "outputId": "bf2d084d-4b3b-45b3-d3fc-3efccaf64d7d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fea787a6690>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#verificar as dimensões do tensor de cada imagem\n",
        "print(imagens[0].shape)\n",
        "#verificar as dimensões do tensor de cada etiqueta\n",
        "print(etiquetas[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7su9ms333T0",
        "outputId": "9074cf4e-99a7-42d7-d051-626bfcafdd62"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 28, 28])\n",
            "torch.Size([])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules.upsampling import F\n",
        "class Modelo(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Modelo, self).__init__()\n",
        "    #camada de entrada, 784 neurônios que se ligam a 128\n",
        "    self.linear1 = nn.Linear(28*28, 128)\n",
        "    #camada interna 1, 128 neurônios que se ligam a 64\n",
        "    self.linear2 = nn.Linear(128, 64)\n",
        "    #camada interna 2, 64 neurônios que se ligam a 10\n",
        "    self.linear3 = nn.Linear(64, 10)\n",
        "    #para cada camada de saída não é necessário definir nada pois só precisamos pegar o output da camada interna 2\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    #função de ativação da camada de entrada para a camada interna 1\n",
        "    x = F.relu(self.linear1(x))\n",
        "    #função de ativação da camada de entrada para a camada interna 2\n",
        "    x = F.relu(self.linear2(x))\n",
        "    #função de ativação da camada de entrada para a camada interna 2 para a camada de saida, nesse caso f(x) = x\n",
        "    return F.log_softmax(x, dim=1)#dados utilizados para calcular a perda"
      ],
      "metadata": {
        "id": "88lWz2XX33ix"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from prompt_toolkit import output\n",
        "def treino(modelo, trainloader, device):\n",
        "  #define a politica de atualização dos pesos e da baias\n",
        "  otimizador = optim.SGD(modelo.parameters(), lr=0.01, momentum=0.5)\n",
        "  #timer para sabermos quanto tempo levou o treino\n",
        "  inicio = time()\n",
        "\n",
        "  #definindo o critério para calcular a perda\n",
        "  criterio = nn.NLLLoss()\n",
        "  EPOCHS = 30 #numero de epochs que o algorismo rodará\n",
        "  modelo.train()#ativando o modo de treinamento do modelo\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "    perda_acumulada = 0 #inicialização da perda acumulada da epoch em questão\n",
        "    for imagens, etiquetas in trainloader:\n",
        "      #convertendo as imagens para vetores de 28*28 casas para ficarem compatíveis\n",
        "      imagens = imagens.view(imagens.shape[0], -1)\n",
        "      otimizador.zero_grad()#zerando os gradientes por conta do ciclo anterior\n",
        "\n",
        "      output = modelo(imagens.to(device)) # colocando os dados no modelo\n",
        "      perda_instantanea = criterio(output, etiquetas.to(device)) # calculando a perda da epoch em questão\n",
        "\n",
        "      perda_instantanea.backward()#back propagation a partir da perda\n",
        "\n",
        "      otimizador.step()#atualizando os pesos e a baias\n",
        "\n",
        "      perda_acumulada += perda_instantanea.item()#atualização da perda acumulada\n",
        "\n",
        "    \n",
        "    else:\n",
        "      print(\"Epoch {} - Perda resultante: {}\".format(epoch+1, perda_acumulada/len(trainloader)))\n",
        "  print(\"\\nTempo de Treino (em minuto) = \",(time()-inicio)/60)"
      ],
      "metadata": {
        "id": "QVs_W8FA33vz"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validacao(modelo, valloader, device):\n",
        "  conta_corretas, conta_todas = 0,0\n",
        "  for imagens, etiquetas in valloader:\n",
        "    for i in range(len(etiquetas)):\n",
        "      img = imagens[1].view(1, 784)\n",
        "      #desativar o autograd para acelerar a validação, grafos computacionais dinâmicos tem um custo alto de processamento\n",
        "      with torch.no_grad():\n",
        "        logps = modelo(img.to(device))\n",
        "\n",
        "\n",
        "      ps = torch.exp(logps) #converte outpu para escala normal(lembrando que é um tensor)\n",
        "      probab = list(ps.cpu().numpy()[0])\n",
        "      #converte o tensor em numero, no caso que o modelo previu como correto\n",
        "      etiqueta_pred = probab.index(max(probab))\n",
        "      etiqueta_certa = etiquetas.numpy()[i]\n",
        "      if(etiqueta_certa == etiqueta_pred):# compara a previsão com o valor correto\n",
        "        conta_corretas += 1\n",
        "      conta_todas += 1\n",
        "\n",
        "  print(\"total de imagens testadas = \", conta_todas)\n",
        "  print(\"\\nPrecisão do modelo = {}\".format(conta_corretas*100/conta_todas))\n"
      ],
      "metadata": {
        "id": "nDISyyBe9G6-"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo = Modelo()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "modelo.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZl6iZfH9HKz",
        "outputId": "d677f28f-75ad-4013-eb46-43807c066590"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Modelo(\n",
              "  (linear1): Linear(in_features=784, out_features=128, bias=True)\n",
              "  (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (linear3): Linear(in_features=64, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tzewvtB_9HZe"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v9PTbOtU9Hm9"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RSgKR9i29H1Z"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tCo-rl1H9IFJ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aF1x06yO9IRN"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}